===========================
简介
===========================


（截至2018年10月11日）

.. contents:: 目录


这是什么
===========================

欢迎来到Spinning Up in Deep RL！这是由OpenAI制作的教育资源，以便更容易地学习深度强化学习（Deep RL）。

对于不熟悉RL的人： `强化学习`_ （RL）是一种机器学习方法，用于教会Agent如何通过反复试验来解决问题。 Deep RL指的是强化学习与 `深度学习`_ 的结合。

该模块包含各种有用的资源，包括：

- 对RL术语，各种算法和基本理论的简短 `介绍`_ （还未完成）
- 关于如何成长为RL研究员的 `论文`_ （还未完成）
- 按主题组织的重要论文 `精选清单`_ （还未完成）
- 关键算法的简短独立实现的详细记录 `代码库`_ （还未完成）
- 以及一些 `练习`_ （还未完成） 作为热身


.. _`强化学习`: https://en.wikipedia.org/wiki/Reinforcement_learning
.. _`深度学习`: http://ufldl.stanford.edu/tutorial/
.. _`介绍`: ../spinningup/rl_intro.html
.. _`论文`: ../spinningup/spinningup.html
.. _`精选清单`: ../spinningup/keypapers.html
.. _`代码库`: https://github.com/openai/spinningup
.. _`练习`: ../spinningup/exercises.html


我们为什么要做这个
===========================

我们听到的一个最常见的问题是

    | 如果我想安全地为AI做出贡献，我该如何开始？

在OpenAI，我们相信，深度学习——特别是深度强化学习——将在强大的AI技术发展中发挥着核心作用。为了确保AI安全，我们必须提出与此范例兼容的安全策略和算法。因此，我们鼓励每个提出这个问题的人来研究这些领域。

然而，尽管有许多资源可以帮助人们快速学习深度学习，但深度强化学习更具挑战性。首先，一位要学习深度强化学习的学生需要具备数学，变成和一定的深度学习的背景知识。除此之外，他们既需要对该领域具有宏观认识——对其中研究的主题的意识，它们的重要性以及已经完成的工作的认识——以及如何将算法理论与算法实现联系起来的指导。

由于该领域非常新颖，因此很难具备宏观认识。目前还没有标准的深度强化学习教材，大部分的知识都还只存在于论文或系列讲座中，这可能需要很长时间才能解析和消化。并且学习实现深度强化学习算法通常都很痛苦，因为：

- 发布算法的论文会忽略或无意中掩盖了一些关键的算法设计细节
- 或者广泛公开的算法实现阅读起来很费劲，看不出如何将代码与算法联系起来的

虽然像 `rllab`_， `Baselines`_ 和 `rllib`_ 这样出色的代码库使得已经在该领域的研究人员更容易取得进步，但是他们将算法构建到这些框架中时，使用了许多不易观察到的选择和权衡，这使得很难从这里学到东西。因此，对于新研究人员以及从业者和业余爱好者来说，深RL领域具有相当高的进入门槛。

因此，这个项目旨在为那些对深度强化学习很感兴趣并且想学习如何使用它或者活出贡献，但是不知从何学起或者不知道如何用代码来实现算法的人做一个中间桥梁。我们争取将这个项目作为一个能对这些人有帮助的起点。

也就是说，从业者并不是唯一能够（或应该）从这些资料中受益的人。安全地解决AI问题需要具有广泛专业知识和观点的人，许多相关专业根本不涉及工程或计算机科学。尽管如此，参与进来的每个人都需要充分了解该技术以做出明智的决策，以及需要几个Spinning Up解决方案。


这如何为我们的使命服务
===========================

OpenAI的 `使命`_ 是确保AGI能够安全地发展以及从AI中获得的收益能够广泛分布。像Spinning Up这样的教学工具帮助我们在这两个目标上取得进展。

首先，只要我们帮助人们了解AI是什么以及它是如何工作的，我们就会更接第二个目标。这使人们能够批判性地思考我们预期会出现的许多问题，因为AI在我们的生活中变得更加复杂和重要。

而且，关键的是， `我们需要人们帮助 <https://jobs.lever.co/openai>`_ 我们努力确保AGI是安全的。由于该领域非常新颖，具备复合技能的人才目前供不应求。我们知道很多人都有兴趣帮助我们，但不知道如何来帮助——那么这就是你应该学习的东西！如果您能成为这方面的专家，您可以在AI安全方面发挥作用。


代码设计理念
===========================

Spinning Up 代码库中的算法实现被设计为

    - 尽可能简单，同时仍然相当不错
    - 并且彼此高度一致以揭示算法之间的基本相似性

它们几乎是完全独立的，互相之间几乎没有共享代码（除了日志记录，保存，加载和MPI实用程序），因此感兴趣的人可以单独研究每个算法，而无需为了查看某事是如何完成的而深入研究连续的依赖关系。这些实现被模式化以使它们尽可能接近伪代码，从而最小化理论和代码之间的距离。

重要的是，它们的结构都是相似的，所以如果你对其中一个能够清楚地理解，那么进行下一个是不会痛苦的。

我们尽可能最小化每个算法实现中使用的技巧数量，并且最小化与其他类似算法之间的差异。这里给出一些没有使用技巧的例子：我们忽略了原始Soft-Actor Critic代码中所用的 `正则化项`_ ，以及所有算法中的 `观察归一化`_ 。关于缩小算法之间差异的示例：我们所实现的的DDPG，TD3和SAC都遵循 `原始TD3代码`_ 中列出的惯例，所有梯度下降更新都在episodes结束时执行（而不是在整个episodes过程中一直更新）。

所有算法都是“相当好”的，因为它们大致达到了预期的性能，但不一定与文献中关于每项任务的最佳结果相匹配。因此，如果要使用任何这些代码实现​​进行科学基准比较的话请务必小心。有关每个实现的效果详细信息，请参阅我们的 `衡量基准`_ 页面。


支持计划
===========================

我们计划支持Spinning Up，以确保它成为学习深度强化学习的有用资源。对Spinning Up的长期（多年）支持尚未确定，但在短期内，我们承诺：

- 发布后前三周的积极支持。

    + 我们将迅速修复错误，回答问答和修改文档以消除歧义。
    + 我们将努力优化用户体验，以便尽可能轻松地使用Spinning Up进行自学。

- 发布后大约六个月（2019年4月），我们将根据我们从社区收到的反馈对该项目的状态进行认真审核，并宣布存在的未来修改计划，包括长期路线图。

此外，正如博客文章中所讨论的，我们正在为即将到来的 `学者`_ 和 `研究员`_ 准备的课程中使用Spinning Up。我们为他们所做的任何更改和更新也会立即公开。


.. _`rllab`: https://github.com/rll/rllab
.. _`Baselines`: https://github.com/openai/baselines
.. _`rllib`: https://github.com/ray-project/ray/tree/master/python/ray/rllib
.. _`使命`: https://blog.openai.com/openai-charter/
.. _`正则化项`: https://github.com/haarnoja/sac/blob/108a4229be6f040360fcca983113df9c4ac23a6a/sac/distributions/normal.py#L69
.. _`观察归一化`: https://github.com/openai/baselines/blob/28aca637d0f13f4415cc5ebb778144154cff3110/baselines/run.py#L131
.. _`原始TD3代码`: https://github.com/sfujim/TD3/blob/25dfc0a6562c54ae5575fad5b8f08bc9d5c4e26c/main.py#L89
.. _`衡量基准`: ../spinningup/bench.html
.. _`学者`: https://jobs.lever.co/openai/cf6de4ed-4afd-4ace-9273-8842c003c842
.. _`研究员`: https://jobs.lever.co/openai/c9ba3f64-2419-4ff9-b81d-0526ae059f57
